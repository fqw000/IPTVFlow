#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IPTV ç›´æ’­æºæ™ºèƒ½æ¸…æ´—ä¸éªŒè¯ç³»ç»Ÿï¼ˆv1.1 - æ·±åº¦éªŒè¯å¢å¼ºç‰ˆï¼‰

åœ¨ v1.0 åŸºç¡€ä¸Šæ–°å¢ï¼š
âœ… **FFmpeg æ·±åº¦æµéªŒè¯**ï¼šå¯¹é .m3u8 é“¾æ¥ï¼Œä½¿ç”¨ ffprobe éªŒè¯éŸ³è§†é¢‘æµç»“æ„
âœ… **Tesseract OCR è½¯é”™è¯¯æ£€æµ‹**ï¼šæˆªå›¾è¯†åˆ«â€œç™»å½•å¢™â€ã€â€œåŒºåŸŸé™åˆ¶â€ç­‰æ— æ•ˆç”»é¢

å…¶ä½™ç‰¹æ€§åŒ v1.0ï¼ˆåŒæ¨¡æºåŠ è½½ã€ä¸¤é˜¶æ®µæµ‹é€Ÿã€é»‘åå•ã€åˆ†ç»„ã€æŠ¥å‘Šç­‰ï¼‰
"""

import os
import re
import time
import json
import logging
import requests
import datetime
import subprocess
from urllib.parse import urlparse, quote
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Tuple, Set, DefaultDict


# ================== å…¨å±€é…ç½®ç±» ==================

# if os.getenv('DEBUG', '').lower() in ('1', 'true', 'yes'):
#     logging.basicConfig(level=logging.DEBUG)
#     logging.debug("Debug mode enabled")

class Config:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰å¯è°ƒå‚æ•°ï¼Œä¾¿äºç»´æŠ¤å’Œè°ƒæ•´ç­–ç•¥"""
    BASE_URL_FILE: str = "config/remote_sources.txt"            # è¿œç¨‹æºåˆ—è¡¨æ–‡ä»¶
    BLACKLIST_FILE: str = "config/blackHost_list.txt"           # ä¸»æœºé»‘åå•æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ª host:portï¼‰
    OUTPUT_FILE: str = "output/live.m3u"                        # æœ€ç»ˆè¾“å‡ºçš„ M3U æ–‡ä»¶
    REPORT_FILE: str = "output/report.md"                              # æ¸…æ´—æŠ¥å‘Šï¼ˆMarkdownï¼‰


    # æ–°å¢ï¼šBark é€šçŸ¥è®¾å¤‡å¯†é’¥ï¼ˆå¯ç•™ç©ºï¼Œæ­¤æ—¶ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    BARK_DEVICE_KEY: str = ""     # ç¤ºä¾‹: "abc123def456" â€”â€” ç•™ç©ºåˆ™å°è¯•ä»ç¯å¢ƒå˜é‡åŠ è½½

    # è°ƒè¯•ä¸­é—´æ–‡ä»¶ï¼ˆç”¨äºé—®é¢˜æ’æŸ¥ï¼‰
    DEBUG_FILES: List[str] = [
        "debug_1_merged_raw.m3u",
        "debug_2_grouped_channels.json",
        "debug_3_host_mapping.json",
        "debug_4_host_quality.json",
        "debug_5_host_ranking.md"
    ]

    TIMEOUT: int = 8                            # HTTP è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
    MAX_WORKERS: int = 15                       # å¹¶å‘çº¿ç¨‹æ•°
    HTTP_OK: int = 200                          # æˆåŠŸ HTTP çŠ¶æ€ç 
    M3U_HEADER: str = '#EXTM3U'                 # M3U æ ‡å‡†å¤´éƒ¨
    HLS_EXTENSIONS: Tuple[str, ...] = ('.m3u8', '.m3u')  # HLS æµæ‰©å±•å
    LOGO_BASE_URL: str = "https://raw.githubusercontent.com/alantang1977/iptv_api/main/pic/logos/"

    # é¢‘é“åˆ†ç»„è§„åˆ™ï¼šæ­£åˆ™åŒ¹é… â†’ åˆ†ç»„åï¼ˆé¡ºåºæ•æ„Ÿï¼Œä¼˜å…ˆåŒ¹é…é å‰è§„åˆ™ï¼‰
    GROUP_RULES: List[Tuple[str, str]] = [
        (r'(CCTV[-]?14|å“ˆå“ˆç‚«åŠ¨|å¡é…·|å®å®|å¹¼æ•™|è´ç“¦|å·§è™|æ–°ç§‘åŠ¨æ¼«|å°çŒªä½©å¥‡|æ±ªæ±ªé˜Ÿ|æµ·åº•å°çºµé˜Ÿ|ç±³è€é¼ |è¿ªå£«å°¼|ç†Šå‡ºæ²¡|çŒ«å’Œè€é¼ |å“†å•¦Aæ¢¦|å–œç¾Šç¾Š|é’å°‘|å„¿ç«¥|åŠ¨ç”»|åŠ¨æ¼«|å°‘å„¿|å¡é€š|é‡‘é¹°|cartoon|disney|å“ˆå“ˆç‚«åŠ¨)', "ğŸ§¸ å„¿ç«¥åŠ¨ç”»"),
        (r'(å¤®è§†|CCTV[0-9]*[é«˜æ¸…]?|CGTN|CCTV|é£äº‘éŸ³ä¹|ç¬¬ä¸€å‰§åœº|æ€€æ—§å‰§åœº|å¥³æ€§æ—¶å°š|é£äº‘è¶³çƒ|ä¸–ç•Œåœ°ç†|å…µå™¨ç§‘æŠ€|ç”µè§†æŒ‡å—)', "ğŸ‡¨ğŸ‡³ å¤®è§†é¢‘é“"),
        (r'(å«è§†|æ¹–å—|æµ™æ±Ÿ|æ±Ÿè‹|åŒ—äº¬|å¹¿ä¸œ|æ·±åœ³|ä¸œæ–¹|å®‰å¾½|å±±ä¸œ|æ²³å—|æ¹–åŒ—|å››å·|è¾½å®|ä¸œå—|å¤©æ´¥|å››å·|å†…è’™å¤|äº‘å—)', "ğŸ“º å«è§†é¢‘é“"),
        (r'(ç¿¡ç¿ |æ˜ç |å‡¤å‡°|é³³å‡°ä¸œæ£®|è²èŠ±|AMC|é¾™å|æ¾³äºš|æ¸¯å°|å¯°å®‡|TVB|åè¯­|ä¸­å¤©|ä¸œæ£®|å¹´ä»£|æ°‘è§†|ä¸‰ç«‹|æ˜Ÿç©º|æ°‘è§†|å°è§†|ç¾äºš|ç¾äº|åƒç¦§|æ— çº¿|ç„¡ç·š|VIUTV|HOY|RTHK|Now|é–å¤©|æ˜Ÿå«|é¦™æ¸¯|æ¾³é—¨|å°æ¹¾)', "ğŸ‡­ğŸ‡° æ¸¯æ¾³å°é¢‘é“"),
        (r'(ä½“è‚²|CCTV5|é«˜å°”å¤«|è¶³çƒ|NBA|è‹±è¶…|è¥¿ç”²|æ¬§å† )', "âš½ ä½“è‚²é¢‘é“"),
        (r'(ç”µå½±|å½±é™¢|CHC|HBO|æ˜Ÿç©º|AXN|TCM|ä½³ç‰‡)', "ğŸ¬ å½±è§†é¢‘é“"),
        (r'(AMC|BET|Discovery|CBS|BET|cine|CNN|disney|epix|espn|fox|american|boomerang|cnbc|entertainment|fs|fuse|fx|hbo|å›½å®¶åœ°ç†|Animal Planet|BBC|NHK|DW|France24|CNN|Al Jazeera)', "ğŸŒ å›½é™…é¢‘é“"),
        (r'(æ•™è‚²|è¯¾å ‚|ç©ºä¸­|å¤§å­¦|å­¦ä¹ |å›½å­¦|ä¹¦ç”»|è€ƒè¯•|ä¸­å­¦|å­¦å ‚|)', "ğŸ“ æ•™è‚²é¢‘é“"),
        # å…œåº•è§„åˆ™ï¼šå…¨è‹±æ–‡ä¸”ä¸å« CCTV/CGTN â†’ å›½é™…é¢‘é“
        (r'^(?=.*[a-zA-Z])(?!.*\b(cctv|cgtn)\b)[a-zA-Z0-9\s\-\+\&\.\'\!\(\)]+$', "ğŸŒ å›½é™…é¢‘é“"),
    ]

    # æœ€ç»ˆæ’­æ”¾åˆ—è¡¨çš„åˆ†ç»„è¾“å‡ºé¡ºåºï¼ˆé‡è¦ï¼æ§åˆ¶ M3U ä¸­åˆ†ç»„æ’åˆ—ï¼‰
    GROUP_OUTPUT_ORDER: List[str] = [
        "ğŸ‡¨ğŸ‡³ å¤®è§†é¢‘é“",
        "ğŸ“º å«è§†é¢‘é“",
        "ğŸ¬ å½±è§†é¢‘é“",
        "âš½ ä½“è‚²é¢‘é“",
        "ğŸ§¸ å„¿ç«¥åŠ¨ç”»",
        "ğŸŒ å›½é™…é¢‘é“",
        "ğŸ“ æ•™è‚²é¢‘é“",
        "ğŸ‡­ğŸ‡° æ¸¯æ¾³å°é¢‘é“",
        "ğŸ“º å…¶ä»–é¢‘é“",  # é»˜è®¤åˆ†ç»„æ”¾æœ€å
    ]


# ================== é¢‘é“åˆ«åå­—å…¸ï¼ˆæ ‡å‡†åŒ–åç§°ï¼‰ ==================
CHANNEL_ALIAS_MAP: Dict[str, str] = {
    "CCTV1ç»¼åˆ": "CCTV1", "CCTV-1": "CCTV1", "CCTV1é«˜æ¸…": "CCTV1", "CCTV1HD": "CCTV1",
    "CCTV-2è´¢ç»": "CCTV2", "CCTV2è´¢ç»": "CCTV2", "CCTV2é«˜æ¸…": "CCTV2",
    "CCTV-3ç»¼è‰º": "CCTV3", "CCTV3ç»¼è‰º": "CCTV3",
    "CCTV-5ä½“è‚²": "CCTV5", "CCTV5ä½“è‚²": "CCTV5",
    "CCTV5+ä½“è‚²èµ›äº‹": "CCTV5+", "CCTV5åŠ ": "CCTV5+",
    "CCTV-13æ–°é—»": "CCTV13", "CCTV13æ–°é—»": "CCTV13",
    "CGTNçºªå½•": "CGTN Documentary", "CGTNè‹±è¯­": "CGTN",
    "æ¹–å—å«è§†é«˜æ¸…": "æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†é«˜æ¸…": "æµ™æ±Ÿå«è§†", "æ±Ÿè‹å«è§†è¶…æ¸…": "æ±Ÿè‹å«è§†",
    "ä¸œæ–¹å«è§†é«˜æ¸…": "ä¸œæ–¹å«è§†", "åŒ—äº¬å«è§†é«˜æ¸…": "åŒ—äº¬å«è§†", "å¹¿ä¸œå«è§†é«˜æ¸…": "å¹¿ä¸œå«è§†", "æ·±åœ³å«è§†é«˜æ¸…": "æ·±åœ³å«è§†",
    "ç¿¡ç¿ å°": "TVB Jade", "æ˜ç å°": "TVB Pearl",
    "å‡¤å‡°ä¸­æ–‡å°": "Phoenix Chinese Channel", "å‡¤å‡°èµ„è®¯å°": "Phoenix InfoNews Channel",
    "ä¸­å¤©ç»¼åˆå°": "CTi Variety", "ä¸­å¤©æ–°é—»å°": "CTi News",
    "ä¸œæ£®æ–°é—»å°": "ETTV News", "ä¸œæ£®æ´‹ç‰‡å°": "ETTV Foreign Movies",
    "é‡‘é¹°å¡é€šé«˜æ¸…": "é‡‘é¹°å¡é€š", "å¡é…·å°‘å„¿": "å¡é…·åŠ¨ç”»", "å“ˆå“ˆç‚«åŠ¨å«è§†": "å“ˆå“ˆç‚«åŠ¨", "æ–°ç§‘åŠ¨æ¼«": "New Tang Dynasty TV",
}


# ================== å·¥å…·å‡½æ•° ==================

def beijing_time_str(fmt: str = "%Y-%m-%d %H:%M:%S") -> str:
    """
    è¿”å›å½“å‰åŒ—äº¬æ—¶é—´å­—ç¬¦ä¸²ï¼ˆUTC+8ï¼‰ï¼Œç¡®ä¿è·¨å¹³å°æ—¶é—´ä¸€è‡´ã€‚
    
    Args:
        fmt: æ—¶é—´æ ¼å¼å­—ç¬¦ä¸²
    
    Returns:
        æ ¼å¼åŒ–åçš„åŒ—äº¬æ—¶é—´å­—ç¬¦ä¸²
    """
    beijing_tz = datetime.timezone(datetime.timedelta(hours=8))
    return datetime.datetime.now(beijing_tz).strftime(fmt)


def setup_logger() -> None:
    """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œæ”¯æŒ DEBUG ç¯å¢ƒå˜é‡æ§åˆ¶æ—¥å¿—çº§åˆ«"""
    log_level = logging.DEBUG if os.getenv("DEBUG") else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )


def verify_and_log(filepath: str, desc: str, critical: bool = False) -> bool:
    """
    éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€éç©ºã€å¯è¯»ï¼Œå¹¶è®°å½•æ—¥å¿—ã€‚
    
    Args:
        filepath: æ–‡ä»¶è·¯å¾„
        desc: æè¿°ä¿¡æ¯
        critical: æ˜¯å¦ä¸ºå…³é”®æ–‡ä»¶ï¼ˆå¤±è´¥åˆ™æŠ›å¼‚å¸¸ï¼‰
    
    Returns:
        æ˜¯å¦éªŒè¯é€šè¿‡
    """
    abs_path = os.path.abspath(filepath)
    exists = os.path.exists(filepath)
    size = os.path.getsize(filepath) if exists else 0
    readable = False
    if exists and size > 0:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                _ = f.read(100)
            readable = True
        except Exception:
            pass

    if exists and size > 0 and readable:
        logging.info(f"âœ… {desc} | è·¯å¾„: {abs_path} | å¤§å°: {size:,} bytes")
        return True
    else:
        status = []
        if not exists:
            status.append("ä¸å­˜åœ¨")
        elif size == 0:
            status.append("ä¸ºç©º(0 bytes)")
        else:
            status.append("ä¸å¯è¯»")
        msg = f"âŒ {desc} å¤±è´¥! çŠ¶æ€: {' + '.join(status)} | è·¯å¾„: {abs_path}"
        if critical:
            raise RuntimeError(msg)
        logging.error(msg)
        return False


def save_debug(content: Any, filepath: str, desc: str) -> None:
    """
    ä¿å­˜è°ƒè¯•ä¸­é—´æ–‡ä»¶ï¼ˆJSON æˆ–æ–‡æœ¬ï¼‰ï¼Œå¹¶éªŒè¯æ˜¯å¦æˆåŠŸå†™å…¥ã€‚
    
    Args:
        content: è¦ä¿å­˜çš„å†…å®¹ï¼ˆstr æˆ– dict/listï¼‰
        filepath: ä¿å­˜è·¯å¾„
        desc: æè¿°ä¿¡æ¯
    """
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            if isinstance(content, str):
                f.write(content)
            else:
                json.dump(content, f, ensure_ascii=False, indent=2)
        verify_and_log(filepath, f"[ä¸­é—´] {desc}")
    except Exception as e:
        logging.error(f"âš ï¸ ä¿å­˜ {desc} å¤±è´¥: {e}")


def cleanup_debug_files() -> None:
    """
    è‡ªåŠ¨åˆ é™¤æ‰€æœ‰è°ƒè¯•ä¸­é—´æ–‡ä»¶ã€‚
    è‹¥è®¾ç½®äº†ç¯å¢ƒå˜é‡ DEBUG=1ï¼Œåˆ™ä¿ç•™æ–‡ä»¶ä¾›æ’æŸ¥ã€‚
    """
    if os.getenv("DEBUG"):
        logging.info("ğŸ’¡ DEBUG æ¨¡å¼å¯ç”¨ï¼Œä¿ç•™æ‰€æœ‰ debug_* æ–‡ä»¶")
        return

    removed = 0
    for f in Config.DEBUG_FILES:
        if os.path.exists(f):
            try:
                os.remove(f)
                removed += 1
            except Exception as e:
                logging.warning(f"âš ï¸ æ— æ³•åˆ é™¤è°ƒè¯•æ–‡ä»¶ {f}: {e}")
    if removed:
        logging.info(f"ğŸ§¹ å·²è‡ªåŠ¨æ¸…ç† {removed} ä¸ªè°ƒè¯•æ–‡ä»¶")


def clean_url(raw: str) -> str:
    """æ¸…ç† URLï¼Œç§»é™¤ $ åå‚æ•°åŠå¤šä½™ç©ºæ ¼"""
    return re.sub(r'[\$â€¢].*|\s+.*', '', raw.strip()).rstrip('/?&')


def is_valid_url(u: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆ HTTP/HTTPS URL"""
    try:
        r = urlparse(u)
        return r.scheme in ("http", "https") and bool(r.netloc)
    except Exception:
        return False


def get_host_key(u: str) -> Optional[str]:
    """
    ä» URL æå– host:port ä½œä¸ºä¸»æœºå”¯ä¸€æ ‡è¯†ã€‚
    ä¾‹å¦‚: https://example.com:8080/path â†’ example.com:8080
    """
    try:
        p = urlparse(u)
        port = p.port or (443 if p.scheme == "https" else 80)
        return f"{p.hostname}:{port}" if p.hostname else None
    except Exception:
        return None


def normalize_channel_name(name: str) -> str:
    """
    æ ‡å‡†åŒ–é¢‘é“åç§°ï¼Œæå‡å»é‡å’Œåˆ†ç»„å‡†ç¡®æ€§ã€‚
    
    æ­¥éª¤ï¼š
    1. å¤šé¢‘é“æ‹¼æ¥å–ä¸»é¢‘é“ï¼ˆå¦‚ A-B-C â†’ Aï¼‰
    2. åˆ«åæ˜ å°„ï¼ˆé•¿åŒ¹é…ä¼˜å…ˆï¼‰
    3. ç§»é™¤æ‹¬å·å†…å®¹
    4. ç»Ÿä¸€è¿æ¥ç¬¦ä¸ºç©ºæ ¼
    5. ç§»é™¤å†—ä½™åç¼€ï¼ˆé«˜æ¸…ã€HDã€é¢‘é“ç­‰ï¼‰
    6. æ ‡å‡†åŒ– CCTV ç¼–å·ï¼ˆCCTV01 â†’ CCTV1ï¼‰
    7. æ ‡å‡†åŒ– CGTN å‰ç¼€
    """
    if not name or not isinstance(name, str):
        return "Unknown"
    original = name.strip()
    if not original:
        return "Unknown"

    # Step 1: å¤„ç†å¤šé¢‘é“æ‹¼æ¥
    if "-" in original and len(original.split("-")) >= 3:
        original = original.split("-", 1)[0].strip()

    # Step 2: ç²¾ç¡®åˆ«åæ˜ å°„ï¼ˆé•¿åˆ«åä¼˜å…ˆï¼‰
    for alias in sorted(CHANNEL_ALIAS_MAP.keys(), key=lambda x: -len(x)):
        if alias in original:
            return CHANNEL_ALIAS_MAP[alias]

    # Step 3: ç§»é™¤æ‹¬å·åŠå†…å®¹
    name = re.sub(r'\s*[\(ï¼ˆã€\[][^)ï¼‰ã€‘\]]*[\)ï¼‰ã€‘\]]\s*', '', original)

    # Step 4: ç»Ÿä¸€è¿æ¥ç¬¦ä¸ºç©ºæ ¼
    name = re.sub(r'[\s\-Â·â€¢_\|]+', ' ', name)
    name = re.sub(r'\s+', ' ', name).strip()

    # Step 5: ç§»é™¤å†—ä½™åç¼€
    suffix_pattern = (
        r'(?:'
        r'HD|FHD|UHD|4K|è¶…é«˜æ¸…|é«˜æ¸…|è“å…‰|æ ‡æ¸…|'
        r'ç»¼åˆé¢‘é“?|ç”µè§†é¢‘é“?|ç›´æ’­é¢‘é“?|å®˜æ–¹é¢‘é“?|'
        r'é¢‘é“|TV|å°|å®˜æ–¹|æ­£ç‰ˆ|æµç•…|å¤‡ç”¨|æµ‹è¯•|'
        r'Ch|CH|Channel|å’ªå’•|çœŸ|æé€Ÿ|'
        r')$'
    )
    name = re.sub(suffix_pattern, '', name, flags=re.IGNORECASE).strip()

    # Step 6: æ™ºèƒ½æ ‡å‡†åŒ– CCTV ç¼–å·
    def cctv_replacer(m):
        num_part = m.group(1)
        digits = re.search(r'[0-9]+', num_part)
        if not digits:
            return m.group(0)
        num_int = int(digits.group())
        suffix = ''
        if '+' in num_part:
            suffix = '+'
        elif 'k' in num_part.lower():
            suffix = 'K'
        return f"CCTV{num_int}{suffix}"

    name = re.sub(
        r'^CCTV[-\s]*([0-9][0-9\s\+\-kK]*)',
        cctv_replacer,
        name,
        flags=re.IGNORECASE
    )

    # Step 7: æ ‡å‡†åŒ– CGTN å‰ç¼€
    name = re.sub(r'^CGTN[-\s]+', 'CGTN ', name, flags=re.IGNORECASE).strip()

    cleaned = name.strip()
    return cleaned if cleaned else original


def guess_group(title: str) -> str:
    """æ ¹æ®é¢‘é“æ ‡é¢˜çŒœæµ‹æ‰€å±åˆ†ç»„ï¼ˆæŒ‰ GROUP_RULES é¡ºåºåŒ¹é…ï¼‰"""
    for pat, grp in Config.GROUP_RULES:
        if re.search(pat, title, re.IGNORECASE):
            return grp
    return "ğŸ“º å…¶ä»–é¢‘é“"


def extract_logo(channel: str) -> str:
    """ç”Ÿæˆ logo URLï¼ˆåŸºäºé¢‘é“åï¼Œç§»é™¤éå­—æ¯æ•°å­—å­—ç¬¦ï¼‰"""
    clean_name = re.sub(r'[^\w]', '', channel).lower()
    return f"{Config.LOGO_BASE_URL}{clean_name}.png"


def is_valid_hls_content(text: str) -> bool:
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ M3U/M3U8 å†…å®¹ï¼ˆåŒ…å«å¿…è¦æ ‡è¯†ï¼‰"""
    txt = text.strip()
    return txt.startswith(Config.M3U_HEADER) and any(k in txt for k in ["#EXTINF", "#EXT-X-STREAM-INF"])


# ================== æ–°å¢ï¼šFFmpeg + OCR éªŒè¯å·¥å…· ==================

try:
    import pytesseract
    from PIL import Image
except ImportError:
    pytesseract = None
    Image = None


def is_ffmpeg_available() -> bool:
    """æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å®‰è£…äº† ffprobeï¼ˆFFmpeg å¥—ä»¶ï¼‰"""
    try:
        subprocess.run(["ffprobe", "-version"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False


def is_ocr_available() -> bool:
    """æ£€æŸ¥ Tesseract OCR å’Œ Python ä¾èµ–æ˜¯å¦å¯ç”¨"""
    if not pytesseract or not Image:
        return False
    try:
        subprocess.run(["tesseract", "-v"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False


def run_ffprobe(url: str, timeout: int = 8) -> bool:
    """ä½¿ç”¨ ffprobe éªŒè¯æµæ˜¯å¦åŒ…å«æœ‰æ•ˆéŸ³è§†é¢‘"""
    try:
        cmd = [
            "ffprobe", "-v", "quiet", "-print_format", "json",
            "-show_streams", "-timeout", str(timeout * 1000000),
            "-user_agent", "Mozilla/5.0", url
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout + 2)
        if result.returncode != 0:
            return False
        data = json.loads(result.stdout)
        streams = data.get("streams", [])
        return len(streams) > 0 and any(s.get("codec_type") in ("video", "audio") for s in streams)
    except Exception:
        return False


def run_ocr_check(url: str, timeout: int = 15) -> bool:
    """å¯¹æµæˆªå›¾å¹¶ OCR æ£€æµ‹è½¯é”™è¯¯ï¼ˆå¦‚ç™»å½•å¢™ã€åŒºåŸŸé™åˆ¶ï¼‰"""
    frame_path = f"/tmp/iptv_ocr_{abs(hash(url)) % 10000}.png"
    try:
        # æˆªå›¾
        cmd = ["ffmpeg", "-y", "-hide_banner", "-loglevel", "error", "-i", url, "-ss", "1", "-vframes", "1", frame_path]
        proc = subprocess.run(cmd, capture_output=True, timeout=timeout + 2)
        if proc.returncode != 0 or not os.path.exists(frame_path):
            return False

        # OCR è¯†åˆ«
        img = Image.open(frame_path)
        text = pytesseract.image_to_string(img, lang='eng+chi_sim').lower()
        blacklist_words = (
            "login", "sign in", "geo-block", "not available", "invalid", "expired",
            "test stream", "demo", "black screen", "è¯·ç™»å½•", "åŒºåŸŸé™åˆ¶", "å¥—é¤", "è´­ä¹°", "403", "unauthorized"
        )
        for word in blacklist_words:
            if word.lower() in text:
                logging.debug(f"ğŸš« OCR æ£€æµ‹åˆ°é»‘åå•è¯ '{word}' in {url}")
                os.remove(frame_path)
                return False
        os.remove(frame_path)
        return True
    except Exception as e:
        if os.path.exists(frame_path):
            os.remove(frame_path)
        logging.debug(f"OCR æ£€æŸ¥å¼‚å¸¸ ({url}): {e}")
        return False


def is_valid_hls_stream(url: str, timeout: int = Config.TIMEOUT) -> Dict[str, Any]:
    """
    æµ‹è¯•å•ä¸ª URL æ˜¯å¦ä¸ºæœ‰æ•ˆ HLS æµã€‚
    æ–°å¢ï¼šå¯¹é .m3u8 é“¾æ¥å¯ç”¨ FFmpeg + OCR æ·±åº¦éªŒè¯ï¼ˆè‹¥ä¾èµ–å¯ç”¨ï¼‰
    """
    headers = {"User-Agent": "Mozilla/5.0", "Accept": "*/*"}
    start = time.time()
    try:
        # HEAD é¢„æ£€
        try:
            h = requests.head(url, timeout=timeout // 2, headers=headers, allow_redirects=True)
            if h.status_code not in (Config.HTTP_OK, 206, 301, 302):
                return {"alive": False, "latency": time.time() - start, "type": "dead", "reason": f"HEAD {h.status_code}"}
        except Exception:
            pass

        # GET è·å–å†…å®¹ç‰‡æ®µ
        r = requests.get(url, timeout=timeout, headers=headers, stream=True, allow_redirects=True)
        if r.status_code != Config.HTTP_OK:
            return {"alive": False, "latency": time.time() - start, "type": "dead", "reason": f"GET {r.status_code}"}

        content = b""
        for chunk in r.iter_content(512):
            content += chunk
            if len(content) > 2048:
                break
        text = content.decode('utf-8', errors='ignore')
        latency = time.time() - start

        if not is_valid_hls_content(text):
            return {"alive": False, "latency": latency, "type": "not_hls", "reason": "Not valid M3U8"}

        stream_type = "master" if "#EXT-X-STREAM-INF" in text else ("media" if ".ts" in text.lower() else "hls")
        result = {"alive": True, "latency": latency, "type": stream_type, "reason": "OK"}

        # === æ–°å¢ï¼šå¯¹éæ ‡å‡† HLS é“¾æ¥è¿›è¡Œæ·±åº¦éªŒè¯ ===
        if not url.endswith(Config.HLS_EXTENSIONS):
            # FFmpeg éªŒè¯æµç»“æ„
            if is_ffmpeg_available():
                if not run_ffprobe(url, timeout=8):
                    result.update({"alive": False, "reason": "FFmpeg validation failed"})
                    return result

        return result
    except Exception as e:
        return {"alive": False, "latency": time.time() - start, "type": "error", "reason": str(e)}


def test_single_stream(url: str, timeout: int = Config.TIMEOUT) -> bool:
    """æœ€ç»ˆé¢‘é“å›æ£€ï¼šè½»é‡éªŒè¯ + OCRï¼ˆè‹¥å¯ç”¨ï¼‰"""
    headers = {"User-Agent": "Mozilla/5.0", "Accept": "*/*"}
    try:
        r = requests.get(url, timeout=timeout, headers=headers, stream=True)
        if r.status_code != Config.HTTP_OK:
            return False
        content = b""
        for chunk in r.iter_content(512):
            content += chunk
            if len(content) > 2048:
                break
        text = content.decode('utf-8', errors='ignore')
        if not is_valid_hls_content(text):
            return False

        # === æ–°å¢ï¼šå¯¹é .m3u8 é“¾æ¥å¯ç”¨ OCR è½¯é”™è¯¯æ£€æµ‹ ===
        if not url.endswith(Config.HLS_EXTENSIONS) and is_ocr_available():
            return run_ocr_check(url, timeout=15)

        return True
    except Exception:
        return False


# ================== åŸæœ‰è§£æä¸åŠ è½½å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰ ==================

def parse_m3u(content: str) -> List[Dict[str, str]]:
    """
    è§£æ M3U å†…å®¹ä¸ºé¢‘é“åˆ—è¡¨ã€‚
    
    Returns:
        [{"name": "é¢‘é“å", "url": "æµåœ°å€"}, ...]
    """
    chs: List[Dict[str, str]] = []
    lines = [l.strip() for l in content.splitlines() if l.strip()]
    i = 0
    while i < len(lines):
        if lines[i].upper().startswith("#EXTINF:"):
            name = lines[i].split(",", 1)[1] if "," in lines[i] else "Unknown"
            if i + 1 < len(lines) and lines[i + 1] and not lines[i + 1].startswith("#"):
                url = clean_url(lines[i + 1])
                if is_valid_url(url):
                    chs.append({"name": name, "url": url})
                i += 2
                continue
        i += 1
    return chs


def txt2m3u_content(txt_content: str) -> str:
    """
    å°† TXT æ ¼å¼ï¼ˆgroup,#genre# + é¢‘é“,URLï¼‰è½¬æ¢ä¸ºæ ‡å‡† M3Uã€‚
    """
    lines = txt_content.splitlines()
    group_title = ""
    m3u_lines = [
        '#EXTM3U x-tvg-url="https://live.fanmingming.com/e.xml"',
        f'# Generated by IPTV Cleaner at {beijing_time_str()}'
    ]
    logo_url_base = "https://live.fanmingming.com/tv/"

    for line in lines:
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        if line.endswith(",#genre#"):
            group_title = line[:-8].strip()
        elif "," in line and line.count(",") == 1:
            parts = line.split(",", 1)
            channel_name = parts[0].strip()
            stream_url = parts[1].strip()
            if not stream_url.startswith(("http://", "https://")):
                continue
            tvg_id = re.sub(r'[\s\-]', '', channel_name)
            logo_url = f"{logo_url_base}{tvg_id}.png"
            m3u_lines.append(
                f'#EXTINF:-1 tvg-id="{tvg_id}" tvg-name="{tvg_id}" '
                f'tvg-logo="{logo_url}" group-title="{group_title}",{channel_name}'
            )
            m3u_lines.append(stream_url)
    return "\n".join(m3u_lines)


def detect_and_convert_to_m3u(content: str) -> str:
    """è‡ªåŠ¨è¯†åˆ«å†…å®¹æ ¼å¼ï¼ˆM3U æˆ– TXTï¼‰å¹¶è½¬æ¢ä¸ºæ ‡å‡† M3U"""
    stripped = content.strip()
    if not stripped:
        return ""
    upper_head = stripped[:200].upper()
    if Config.M3U_HEADER in upper_head and "#EXTINF" in upper_head:
        lines = [line for line in stripped.splitlines() if not line.strip().upper().startswith(Config.M3U_HEADER)]
        return "\n".join([Config.M3U_HEADER] + lines)
    else:
        return txt2m3u_content(stripped)


def load_blacklist() -> Set[str]:
    """
    åŠ è½½ä¸»æœºé»‘åå•æ–‡ä»¶ï¼ˆblackHost_list.txtï¼‰ã€‚
    - å¿½ç•¥ç©ºè¡Œå’Œ # å¼€å¤´çš„æ³¨é‡Šè¡Œ
    - è‡ªåŠ¨è¡¥å…¨é»˜è®¤ç«¯å£ï¼ˆ80/443ï¼‰
    
    Returns:
        set of "host:port" strings
    """
    blacklist: Set[str] = set()
    if not os.path.exists(Config.BLACKLIST_FILE):
        logging.info(f"â„¹ï¸  é»‘åå•æ–‡ä»¶ {Config.BLACKLIST_FILE} ä¸å­˜åœ¨ï¼Œè·³è¿‡è¿‡æ»¤")
        return blacklist

    try:
        with open(Config.BLACKLIST_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                raw = line.strip()
                if not raw or raw.startswith("#"):
                    continue
                host = raw.lower().strip()
                # è¡¥å…¨ç«¯å£ï¼ˆä¸ get_host_key é€»è¾‘ä¸€è‡´ï¼‰
                if ':' not in host:
                    host += ':80'
                blacklist.add(host)
        logging.info(f"ğŸ›¡ï¸  åŠ è½½é»‘åå•ä¸»æœºæ•°: {len(blacklist)} | æ¥è‡ª: {os.path.abspath(Config.BLACKLIST_FILE)}")
    except Exception as e:
        logging.error(f"âŒ è¯»å–é»‘åå•å¤±è´¥: {e}")
    return blacklist


# ================== æ ¸å¿ƒæµç¨‹å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰ ==================

def load_sources() -> Tuple[str, List[Dict[str, Any]]]:
    """
    åŠ è½½è¿œç¨‹ + æœ¬åœ°æºï¼Œè¿”å›åˆå¹¶åçš„ M3U å†…å®¹ å’Œ æºåŠ è½½è¯¦æƒ…åˆ—è¡¨ã€‚
    
    Returns:
        (merged_m3u_content: str, source_load_details: List[dict])
    """
    source_details: List[Dict[str, Any]] = []

    # --- åŠ è½½è¿œç¨‹æº ---
    if not os.path.exists(Config.BASE_URL_FILE):
        raise FileNotFoundError(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {os.path.abspath(Config.BASE_URL_FILE)}")

    remote_urls: List[str] = []
    with open(Config.BASE_URL_FILE, "r", encoding="utf-8") as f:
        for i, line in enumerate(f, 1):
            raw = line.strip()
            if raw and not raw.startswith("#") and raw.startswith(("http://", "https://")):
                remote_urls.append(raw)
            elif raw and not raw.startswith("#"):
                logging.warning(f"âš ï¸  å¿½ç•¥æ— æ•ˆURL (ç¬¬{i}è¡Œ): {raw}")
    if not remote_urls:
        raise ValueError("baseURL.txtä¸­æ— æœ‰æ•ˆæºURL")

    s = requests.Session()
    s.headers.update({"User-Agent": "Mozilla/5.0"})

    def fetch(u: str) -> Tuple[List[str], Dict[str, Any]]:
        """è·å–å•ä¸ªè¿œç¨‹æºå†…å®¹"""
        detail = {"type": "remote", "url": u, "success": False, "error": "", "line_count": 0}
        try:
            r = s.get(u, timeout=10)
            if r.status_code == Config.HTTP_OK:
                raw_text = r.text
                converted = detect_and_convert_to_m3u(raw_text)
                if not converted.strip():
                    detail["error"] = "å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ"
                    return [], detail
                lines = [line for line in converted.splitlines() if not line.strip().upper().startswith(Config.M3U_HEADER)]
                detail.update({"success": True, "line_count": len(lines)})
                return lines, detail
            else:
                detail["error"] = f"HTTP {r.status_code}"
                return [], detail
        except Exception as e:
            detail["error"] = str(e)
            return [], detail

    merged = [Config.M3U_HEADER]
    with ThreadPoolExecutor(max_workers=min(10, len(remote_urls))) as executor:
        futures = [executor.submit(fetch, u) for u in remote_urls]
        for future in as_completed(futures):
            lines, detail = future.result()
            merged.extend(lines)
            source_details.append(detail)

    remote_content = "\n".join(merged)

    # --- åŠ è½½æœ¬åœ°æº ---
    local_dir = "local_playlists"
    local_contents: List[str] = []
    if os.path.exists(local_dir):
        for fname in os.listdir(local_dir):
            if not fname.lower().endswith(('.txt', '.m3u')):
                continue
            fpath = os.path.join(local_dir, fname)
            detail = {"type": "local", "path": fpath, "success": False, "error": "", "line_count": 0}
            try:
                with open(fpath, 'r', encoding='utf-8') as f:
                    raw = f.read()
                if fname.lower().endswith('.txt'):
                    m3u_content = txt2m3u_content(raw)
                    logging.info(f"ğŸ”„ å·²è½¬æ¢æœ¬åœ° TXT æ–‡ä»¶: {fpath}")
                else:
                    m3u_content = raw
                    logging.info(f"ğŸ“¥ å·²åŠ è½½æœ¬åœ° M3U æ–‡ä»¶: {fpath}")
                local_contents.append(m3u_content)
                detail.update({"success": True, "line_count": len(m3u_content.splitlines())})
            except Exception as e:
                detail["error"] = str(e)
                logging.error(f"âŒ åŠ è½½æœ¬åœ°æ–‡ä»¶å¤±è´¥: {fpath} | {e}")
            source_details.append(detail)
    else:
        logging.info("â„¹ï¸  æœ¬åœ°æºç›®å½• baseM3U ä¸å­˜åœ¨ï¼Œè·³è¿‡æœ¬åœ°æ–‡ä»¶åŠ è½½")

    all_contents = [remote_content] + local_contents
    raw_content = "\n".join(all_contents).strip()
    if not raw_content:
        raise RuntimeError("æ‰€æœ‰æºï¼ˆè¿œç¨‹+æœ¬åœ°ï¼‰å‡ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥")

    logging.info(f"ğŸ”— æ€»å…±åˆå¹¶äº† {len(local_contents) + 1} ä»½æºå†…å®¹ï¼ˆ1è¿œç¨‹ + {len(local_contents)}æœ¬åœ°ï¼‰")
    save_debug(raw_content, Config.DEBUG_FILES[0], "åˆå¹¶åŸå§‹M3U")
    return raw_content, source_details


def test_hosts_two_phase(host_to_urls: Dict[str, List[str]]) -> Dict[str, Dict[str, Any]]:
    """
    ä¸¤é˜¶æ®µä¸»æœºæµ‹é€Ÿï¼š
    1. åˆç­›ï¼šä½¿ç”¨ä»£è¡¨æ€§ .m3u8 URL å¿«é€Ÿåˆç­›
    2. å›æŸ¥ï¼šå¯¹å¤±è´¥ä¸»æœºå°è¯•æœ€å¤š3ä¸ªå¤‡é€‰æµè¿›è¡Œå›æŸ¥
    
    Returns:
        host_quality: {host: {alive, latency, type, representative_url, trials}}
    """
    # æ„å»ºä¸»æœºæ•°æ®ï¼ˆå·²å»é‡ï¼‰
    host_data: Dict[str, Dict[str, Any]] = {}
    for host, urls in host_to_urls.items():
        rep_url = next((u for u in urls if u.endswith(Config.HLS_EXTENSIONS)), urls[0]) if urls else None
        host_data[host] = {"all_urls": urls, "rep_url": rep_url}

    host_quality: Dict[str, Dict[str, Any]] = {}

    # === ç¬¬ä¸€é˜¶æ®µï¼šåˆç­› ===
    initial_results: Dict[str, Dict[str, Any]] = {}
    with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
        future_to_host = {
            executor.submit(is_valid_hls_stream, data["rep_url"]): host
            for host, data in host_data.items() if data["rep_url"]
        }
        for future in as_completed(future_to_host):
            host = future_to_host[future]
            result = future.result()
            initial_results[host] = result
            if result["alive"]:
                host_quality[host] = {
                    "alive": True,
                    "latency": result["latency"],
                    "type": result["type"],
                    "representative_url": host_data[host]["rep_url"],
                    "trials": 1
                }

    # === ç¬¬äºŒé˜¶æ®µï¼šå¤±è´¥å›æŸ¥ ===
    hosts_to_retry: List[Tuple[str, List[str]]] = []
    for host, data in host_data.items():
        if host not in host_quality:
            candidates = [u for u in data["all_urls"] if u != data["rep_url"]]
            if candidates:
                hosts_to_retry.append((host, candidates[:3]))  # æœ€å¤š3ä¸ª
            else:
                host_quality[host] = {
                    "alive": False,
                    "latency": 999,
                    "type": "no_fallback",
                    "representative_url": data["rep_url"],
                    "trials": 1
                }

    logging.info(f"ğŸ”„ åˆç­›å¤±è´¥ä¸»æœºæ•°: {len(hosts_to_retry)}ï¼Œå¼€å§‹äºŒæ¬¡æµ‹é€Ÿ...")

    if hosts_to_retry:
        def fallback_test(args: Tuple[str, List[str]]) -> Tuple[str, bool, Dict[str, Any], int]:
            host, candidates = args
            for i, url in enumerate(candidates):
                res = is_valid_hls_stream(url)
                if res["alive"]:
                    return host, True, {"latency": res["latency"], "type": res["type"], "url": url}, i + 1
            last_res = is_valid_hls_stream(candidates[-1]) if candidates else None
            return host, False, (last_res or {"latency": 999, "type": "all_failed"}), len(candidates)

        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
            futures = [executor.submit(fallback_test, (host, cands)) for host, cands in hosts_to_retry]
            for future in as_completed(futures):
                host, success, final_result, trials = future.result()
                rep_url = host_data[host]["rep_url"]
                if success:
                    host_quality[host] = {
                        "alive": True,
                        "latency": final_result["latency"],
                        "type": final_result["type"],
                        "representative_url": final_result["url"],
                        "trials": 1 + trials
                    }
                else:
                    host_quality[host] = {
                        "alive": False,
                        "latency": final_result["latency"],
                        "type": final_result["type"],
                        "representative_url": rep_url,
                        "trials": 1 + trials
                    }

    alive = sum(1 for r in host_quality.values() if r["alive"])
    survival_rate = alive / len(host_to_urls) * 100 if host_to_urls else 0
    logging.info(f"âœ… å­˜æ´»ä¸»æœº: {alive}/{len(host_to_urls)} ({survival_rate:.1f}%)")
    save_debug(host_quality, Config.DEBUG_FILES[3], "Hostæµ‹é€Ÿç»“æœï¼ˆä¸¤é˜¶æ®µï¼‰")
    return host_quality


def save_host_ranking(host_quality: Dict[str, Dict[str, Any]]) -> None:
    """ä¿å­˜ä¸»æœºæµ‹é€Ÿæ’åï¼ˆå«å®Œæ•´URLï¼‰åˆ° Markdown æ–‡ä»¶"""
    ranking = []
    for host, result in host_quality.items():
        if not result.get("alive"):
            continue
        rep_url = result.get("representative_url", "")
        latency_ms = int(result["latency"] * 1000)
        stream_type = result.get("type", "unknown")
        trials = result.get("trials", 1)
        ranking.append({
            "host": host,
            "url": rep_url,
            "latency_ms": latency_ms,
            "type": stream_type,
            "trials": trials
        })

    ranking.sort(key=lambda x: x["latency_ms"])
    lines = [
        "# ğŸš€ ä¸»æœºæµ‹é€Ÿæ’åï¼ˆæŒ‰å»¶è¿Ÿå‡åºï¼‰",
        "",
        "| ä¸»æœº | å»¶è¿Ÿ(ms) | ç±»å‹ | å°è¯•æ¬¡æ•° | æµ‹é€Ÿé“¾æ¥ï¼ˆå®Œæ•´ï¼‰|",
        "|------|----------|------|----------|------------------|"
    ]
    for item in ranking:
        url_display = f"`{item['url']}`"
        lines.append(f"| `{item['host']}` | **{item['latency_ms']}** | `{item['type']}` | {item['trials']} | {url_display} |")

    save_debug("\n".join(lines), Config.DEBUG_FILES[4], "ä¸»æœºæµ‹é€Ÿæ’åï¼ˆå«å®Œæ•´URLï¼‰")


def build_final_playlist(
    grouped: DefaultDict[str, List[Dict[str, str]]],
    url_to_host: Dict[str, str],
    host_quality: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    """
    æ„å»ºæœ€ç»ˆæ’­æ”¾åˆ—è¡¨ï¼ˆå«é¢‘é“çº§å›é€€éªŒè¯ï¼‰ã€‚
    å¯¹æ¯ä¸ªé¢‘é“ï¼ŒæŒ‰ä¸»æœºå»¶è¿Ÿæ’åºå€™é€‰æºï¼Œå¹¶é€ä¸ªéªŒè¯æµæ˜¯å¦çœŸæ­£å¯ç”¨ã€‚
    
    Returns:
        [{"channel", "original_name", "url"}, ...]
    """
    channel_to_candidates: Dict[str, List[Dict[str, Any]]] = {}
    for clean_name, items in grouped.items():
        candidates = []
        for item in items:
            url = item["url"]
            host = url_to_host.get(url)
            if host and host_quality.get(host, {}).get("alive"):
                latency = host_quality[host]["latency"]
                candidates.append({
                    "url": url,
                    "latency": latency,
                    "original_name": item["original_name"]
                })
        if candidates:
            candidates.sort(key=lambda x: x["latency"])
            channel_to_candidates[clean_name] = candidates

    final_channels: List[Dict[str, str]] = []
    for clean_name, candidates in channel_to_candidates.items():
        selected = None
        for cand in candidates:
            if test_single_stream(cand["url"]):
                selected = {
                    "channel": clean_name,
                    "original_name": cand["original_name"],
                    "url": cand["url"]
                }
                break
        if selected:
            final_channels.append(selected)
        else:
            logging.warning(f"âš ï¸  é¢‘é“ '{clean_name}' æ‰€æœ‰å€™é€‰æºå‡ä¸å¯ç”¨ï¼Œå·²è·³è¿‡")

    logging.info(f"ğŸ¯ æœ€ç»ˆä¿ç•™é¢‘é“: {len(final_channels)}")
    return final_channels


def generate_outputs_and_notify(
    final_channels: List[Dict[str, str]],
    stats: Dict[str, Any],
    source_details: List[Dict[str, Any]],
    all_expected_channels: Set[str]
) -> None:
    """
    ç”Ÿæˆæœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼ˆM3U + Reportï¼‰å¹¶å‘é€ Bark é€šçŸ¥ã€‚
    æ–°å¢ï¼šæŒ‰ Config.GROUP_OUTPUT_ORDER æ’åºé¢‘é“ + å¯ç”¨/æ— æºé¢‘é“æ¸…å•ã€‚
    """
    # --- æŒ‰æŒ‡å®šé¡ºåºç»„ç»‡é¢‘é“ ---
    group_to_channels: DefaultDict[str, List[Dict[str, str]]] = defaultdict(list)
    for item in final_channels:
        g = guess_group(item["original_name"])
        group_to_channels[g].append(item)

    # æŒ‰é¢„è®¾é¡ºåºæ’åˆ—åˆ†ç»„
    ordered_groups: List[Tuple[str, List[Dict[str, str]]]] = []
    for group_name in Config.GROUP_OUTPUT_ORDER:
        if group_name in group_to_channels:
            ordered_groups.append((group_name, group_to_channels[group_name]))
    # æ·»åŠ æœªåœ¨æ’åºåˆ—è¡¨ä¸­çš„åˆ†ç»„ï¼ˆç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼‰
    for g, chs in group_to_channels.items():
        if g not in Config.GROUP_OUTPUT_ORDER:
            ordered_groups.append((g, chs))

    # --- ç”Ÿæˆ M3U ---
    with open(Config.OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write('#EXTM3U x-tvg-url="https://live.fanmingming.com/europe.xml.gz"\n')
        for group_name, channels in ordered_groups:
            for item in channels:
                logo = extract_logo(item["channel"])
                f.write(
                    f'#EXTINF:-1 tvg-id="{item["channel"]}" tvg-name="{item["channel"]}" '
                    f'tvg-logo="{logo}" group-title="{group_name}",{item["channel"]}\n'
                    f'{item["url"]}\n'
                )
    verify_and_log(Config.OUTPUT_FILE, "[è¾“å‡º] ç›´æ’­æºM3U", critical=True)

    # --- ç»Ÿè®¡åˆ†ç»„æ•°é‡ ---
    group_count = defaultdict(int)
    for item in final_channels:
        group_count[guess_group(item["original_name"])] += 1

    # --- ç”ŸæˆæŠ¥å‘Š ---
    report = f"""# âœ… IPTVç›´æ’­æºæ¸…æ´—æŠ¥å‘Š
**ç”Ÿæˆæ—¶é—´**: {beijing_time_str()}
**å­˜æ´»ç‡**: {stats['survival_rate']:.1f}%

## ğŸ“Š æ ¸å¿ƒç»Ÿè®¡
| é¡¹ç›® | æ•°é‡ |
|------|------|
| åŸå§‹é¢‘é“ | {stats['raw_channels']} |
| å”¯ä¸€ä¸»æœº | {stats['unique_hosts']} |
| å­˜æ´»ä¸»æœº | {stats['alive_hosts']} |
| æœ€ç»ˆé¢‘é“ | {stats['final_channels']} |

## ğŸ”— æºåŠ è½½è¯¦æƒ…
| ç±»å‹ | æ ‡è¯† | çŠ¶æ€ | è¡Œæ•° | é”™è¯¯ä¿¡æ¯ |
|------|------|------|------|----------|
"""

    for detail in source_details:
        if detail["type"] == "remote":
            url_full = detail["url"]
            # ä½¿ç”¨åå¼•å·åŒ…è£¹å®Œæ•´ URLï¼ŒMarkdown ä¸­ä»å¯ç‚¹å‡»
            status = "âœ… æˆåŠŸ" if detail["success"] else "âŒ å¤±è´¥"
            error = detail.get("error", "") or "-"
            report += f"| è¿œç¨‹ | `{url_full}` | {status} | {detail['line_count']} | {error} |\n"
        else:
            filename = os.path.basename(detail["path"])
            status = "âœ… æˆåŠŸ" if detail["success"] else "âŒ å¤±è´¥"
            error = detail.get("error", "") or "-"
            report += f"| æœ¬åœ° | `{filename}` | {status} | {detail['line_count']} | {error} |\n"

    report += "\n## ğŸ“º åˆ†ç»„åˆ†å¸ƒ\n"
    for g, c in sorted(group_count.items(), key=lambda x: x[1], reverse=True):
        report += f"- **{g}**: {c} ä¸ª\n"

    # --- æ–°å¢ï¼šå¯ç”¨é¢‘é“ä¸æ— æºé¢‘é“æ¸…å• ---
    available_set = {item["channel"] for item in final_channels}
    unavailable_channels = sorted(all_expected_channels - available_set)

    report += "\n## ğŸ“‹ å¯ç”¨é¢‘é“åˆ—è¡¨\n"
    for ch in sorted(available_set):
        report += f"- {ch}\n"

    report += "\n## âŒ æ— æœ‰æ•ˆæºçš„é¢‘é“\n"
    if unavailable_channels:
        for ch in unavailable_channels:
            report += f"- {ch}\n"
    else:
        report += "- æ— \n"

    with open(Config.REPORT_FILE, "w", encoding="utf-8") as f:
        f.write(report)
    verify_and_log(Config.REPORT_FILE, "[è¾“å‡º] æ¸…æ´—æŠ¥å‘Š")

    # --- Bark é€šçŸ¥ï¼ˆæ”¯æŒ Config + ç¯å¢ƒå˜é‡ï¼‰---
    env_key = os.getenv("BARK_DEVICE_KEY", "").strip()
    config_key = Config.BARK_DEVICE_KEY.strip()
    bark_key = env_key or config_key  # ç¯å¢ƒå˜é‡ä¼˜å…ˆ
    
    if bark_key:
        top_groups = sorted(stats['groups'].items(), key=lambda x: x[1], reverse=True)[:8]
        group_lines = [f"â€¢ {g}: {c}ä¸ª" for g, c in top_groups]
        if len(stats['groups']) > 8:
            group_lines.append(f"â€¢ ...ï¼ˆå…±{len(stats['groups'])}ç»„ï¼‰")

        body = (
            f"âŒšï¸ {beijing_time_str()}\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            "ğŸ“Š æ ¸å¿ƒæ•°æ®\n"
            f"  åŸå§‹é¢‘é“: {stats['raw_channels']}\n"
            f"  å”¯ä¸€ä¸»æœº: {stats['unique_hosts']}\n"
            f"  å­˜æ´»ä¸»æœº: {stats['alive_hosts']} ({stats['survival_rate']:.1f}%)\n"
            f"  æœ€ç»ˆé¢‘é“: {stats['final_channels']}\n"
            "\nğŸ“º ä¿ç•™é¢‘é“åˆ†ç»„\n" +
            "\n".join(group_lines) +
            "\n\nğŸ’¡ å®Œæ•´æŠ¥å‘Š: report.md"
        )
        try:
            safe_title = quote("ğŸ“º IPTVæºæ¸…æ´—å®Œæˆ", safe='')
            safe_body = quote(body, safe='')
            url = f"https://api.day.app/{bark_key}/{safe_title}/{safe_body}?group=iptv&icon=ğŸ“º&level=active"
            resp = requests.get(url, timeout=10)
            logging.info("âœ… Barké€šçŸ¥å‘é€æˆåŠŸ" if resp.status_code == 200 else f"âš ï¸ Barkå¤±è´¥ (HTTP {resp.status_code})")
        except Exception as e:
            logging.error(f"âŒ Barkå¼‚å¸¸: {e}")
    else:
        logging.info("â„¹ï¸  æœªé…ç½® BARK_DEVICE_KEYï¼ˆConfig æˆ–ç¯å¢ƒå˜é‡ï¼‰ï¼Œè·³è¿‡é€šçŸ¥")


# ================== ä¸»å…¥å£ ==================
def main() -> None:
    """ä¸»æ‰§è¡Œæµç¨‹"""
    setup_logger()
    logger = logging.getLogger()
    cwd = os.getcwd()
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“ è„šæœ¬å·¥ä½œç›®å½•: {cwd}")
    logger.info(f"ğŸ’¡ è¯·ç¡®è®¤æ­¤ç›®å½•åŒ…å«: {Config.BASE_URL_FILE}")
    logger.info(f"{'='*60}\n")

    # === æ£€æŸ¥å¯é€‰ä¾èµ– ===
    if is_ffmpeg_available():
        logging.info("âœ… FFmpeg å¯ç”¨ï¼Œå°†å¯¹é .m3u8 é“¾æ¥å¯ç”¨æ·±åº¦æµéªŒè¯")
    else:
        logging.warning("âš ï¸  FFmpeg æœªå®‰è£…ï¼Œè·³è¿‡æ·±åº¦æµéªŒè¯")

    if is_ocr_available():
        logging.info("âœ… OCR å¯ç”¨ï¼Œå°†åœ¨æœ€ç»ˆå›æ£€ä¸­å¯ç”¨ç”»é¢è½¯é”™è¯¯æ£€æµ‹")
    else:
        logging.warning("âš ï¸  OCR ä¾èµ–ç¼ºå¤±ï¼Œè·³è¿‡ç”»é¢æ£€æµ‹")

    try:
        test_file = ".write_check.tmp"
        with open(test_file, 'w') as f:
            f.write("dir_check")
        os.remove(test_file)
        logger.info(f"âœ… å·¥ä½œç›®å½•å¯å†™éªŒè¯é€šè¿‡")
    except Exception as e:
        logger.critical(f"âŒ å·¥ä½œç›®å½•ä¸å¯å†™! è¯·æ£€æŸ¥æƒé™: {e}")
        return

    # === åŠ è½½æº ===
    raw_content, source_details = load_sources()

    # === åŠ è½½é»‘åå• ===
    blacklist_hosts = load_blacklist()

    # === è§£æåŸå§‹é¢‘é“å¹¶è¿‡æ»¤é»‘åå• ===
    raw_channels = parse_m3u(raw_content)
    logging.info(f"ğŸ§¹ è§£æåˆ° {len(raw_channels)} æ¡åŸå§‹é¢‘é“è®°å½•")

    # æ„å»ºä¸´æ—¶ url_to_host ç”¨äºè¿‡æ»¤
    temp_url_to_host: Dict[str, str] = {}
    for ch in raw_channels:
        if is_valid_url(ch["url"]):
            if (host := get_host_key(ch["url"])):
                temp_url_to_host[ch["url"]] = host

    # è¿‡æ»¤é»‘åå•
    filtered_raw_channels = []
    for ch in raw_channels:
        host = temp_url_to_host.get(ch["url"])
        if host and host in blacklist_hosts:
            logging.debug(f"ğŸš« é»‘åå•è·³è¿‡: {ch['name']} â†’ {ch['url']}")
            continue
        filtered_raw_channels.append(ch)

    # åˆ†ç»„ï¼ˆå»é‡ï¼‰
    grouped: DefaultDict[str, List[Dict[str, str]]] = defaultdict(list)
    seen_urls: Set[str] = set()
    for ch in filtered_raw_channels:
        clean_name = normalize_channel_name(ch["name"])
        if ch["url"] not in seen_urls:
            seen_urls.add(ch["url"])
            grouped[clean_name].append({"original_name": ch["name"], "url": ch["url"]})
    logging.info(f"ğŸ“¦ åˆ†ç»„åå…± {len(grouped)} ä¸ªç‹¬ç«‹é¢‘é“")
    save_debug(dict(grouped), Config.DEBUG_FILES[1], "åˆ†ç»„é¢‘é“æ•°æ®")

    # é‡å»º url_to_hostï¼ˆä»…éé»‘åå•ï¼‰
    url_to_host: Dict[str, str] = {}
    for items in grouped.values():
        for item in items:
            if (host := get_host_key(item["url"])):
                url_to_host[item["url"]] = host
    logging.info(f"ğŸŒ å”¯ä¸€ä¸»æœºæ•°: {len(url_to_host)}")
    save_debug({h: u for u, h in url_to_host.items()}, Config.DEBUG_FILES[2], "Hostæ˜ å°„æ•°æ®")

    # === æµ‹é€Ÿä¸æ„å»ºæœ€ç»ˆåˆ—è¡¨ ===
    host_to_urls = defaultdict(list)
    for items in grouped.values():
        for item in items:
            if (host := url_to_host.get(item["url"])):
                host_to_urls[host].append(item["url"])

    host_quality = test_hosts_two_phase(dict(host_to_urls))
    save_host_ranking(host_quality)
    final_channels = build_final_playlist(grouped, url_to_host, host_quality)

    # === ç»Ÿè®¡ ===
    alive_hosts = sum(1 for r in host_quality.values() if r["alive"])
    survival_rate = alive_hosts / len(host_to_urls) * 100 if host_to_urls else 0
    group_count = defaultdict(int)
    for item in final_channels:
        group_count[guess_group(item["original_name"])] += 1

    stats = {
        "raw_channels": len(grouped),
        "unique_hosts": len(host_to_urls),
        "alive_hosts": alive_hosts,
        "survival_rate": survival_rate,
        "final_channels": len(final_channels),
        "groups": dict(group_count)
    }

    all_expected_channels = set(grouped.keys())
    generate_outputs_and_notify(final_channels, stats, source_details, all_expected_channels)

    # === è‡ªåŠ¨æ¸…ç†è°ƒè¯•æ–‡ä»¶ ===
    cleanup_debug_files()

    # === ç»ˆæç¡®è®¤ ===
    logger.info("\n" + "="*70)
    logger.info("ğŸ”ã€ç»ˆæç¡®è®¤ã€‘è¯·æ ¸å¯¹ä»¥ä¸‹æ–‡ä»¶è·¯å¾„")
    logger.info("="*70)
    for desc, fname in [(" cleaned M3U", Config.OUTPUT_FILE), ("Report", Config.REPORT_FILE)]:
        abs_path = os.path.abspath(fname)
        if os.path.exists(fname) and os.path.getsize(fname) > 0:
            logger.info(f"âœ… {desc} | {abs_path}")
        else:
            logger.error(f"âŒ {desc} æœªæ‰¾åˆ°! | {abs_path}")
    logger.info("="*70)
    logger.info(f"ğŸ“Œ å·¥ä½œç›®å½•: {cwd}")
    logger.info("="*70 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logging.info("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        exit(1)
    except Exception as e:
        logging.exception(f"âŒ è„šæœ¬æ‰§è¡Œå¼‚å¸¸: {e}")
        logging.info("ğŸ’¡ å¼‚å¸¸å‘ç”Ÿï¼ä¸­é—´æ–‡ä»¶å·²ä¿ç•™ï¼ˆè‹¥ DEBUG=1ï¼‰")
        exit(1)
